import io
import re
import pdfplumber
from transformers import GPT2TokenizerFast
import openai
from typing import Dict, List
from typing import TypedDict, Annotated
from langchain_core.messages import BaseMessage
import operator
from langchain_core.tools import tool


client = openai.OpenAI(api_key = "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")
# TypedDictë¥¼ ì‚¬ìš©í•˜ì—¬ ì—ì´ì „íŠ¸ì˜ 'ìƒíƒœ' ë˜ëŠ” 'ê¸°ì–µ'ì˜ êµ¬ì¡°ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
class AgentState(TypedDict):
    # 'messages' í‚¤ì—ëŠ” ëŒ€í™” ê¸°ë¡ì´ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì €ì¥ë©ë‹ˆë‹¤.
    # operator.addëŠ” ìƒˆ ë©”ì‹œì§€ê°€ ë“¤ì–´ì˜¬ ë•Œë§ˆë‹¤ ê¸°ì¡´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.
    messages: Annotated[List[BaseMessage], operator.add]
    
    # í˜„ì¬ ëŒ€í™”ì˜ ëŒ€ìƒì´ ë˜ëŠ” íŠ¹í—ˆ ë¬¸ì„œì˜ ì „ì²´ ë‚´ìš©ì„ ì €ì¥í•©ë‹ˆë‹¤.
    patent_document: dict


# ğŸ“‹ í•„ìˆ˜ ì„¹ì…˜ ëˆ„ë½ íƒì§€
REQUIRED_SECTIONS = {
    "ë°œëª…ì˜ ëª…ì¹­": ["ë°œëª…ì˜ ëª…ì¹­", "ë°œëª…ì˜ ì œëª©"],
    "ê¸°ìˆ  ë¶„ì•¼": ["ê¸°ìˆ  ë¶„ì•¼", "ê¸°ìˆ ë¶„ì•¼","ê¸° ìˆ  ë¶„ ì•¼"],
    "ë°°ê²½ ê¸°ìˆ ": ["ë°°ê²½ ê¸°ìˆ ", "ì¢…ë˜ ê¸°ìˆ ", "ì¢…ë˜ê¸°ìˆ ", "ê¸°ì¡´ ê¸°ìˆ ","ë°° ê²½ ê¸° ìˆ "],
    "ë°œëª…ì˜ ë‚´ìš©": ["ë°œëª…ì˜ ë‚´ìš©", "í•´ê²° ìˆ˜ë‹¨", "íš¨ê³¼"],
    "ë„ë©´ì˜ ê°„ë‹¨í•œ ì„¤ëª…": ["ë„ë©´ì˜ ê°„ë‹¨í•œ ì„¤ëª…", "ê°„ë‹¨í•œ ë„ë©´ ì„¤ëª…", "ë„ë©´ ì„¤ëª…"],
    "ë°œëª…ì˜ ì‹¤ì‹œì˜ˆ": ["ì‹¤ì‹œì˜ˆ", "ì‹¤ì‹œ í˜•íƒœ", "ì‹¤ì‹œ ë°©ì‹"],
    "ì²­êµ¬í•­": ["ì²­êµ¬í•­ 1", "ì²­êµ¬í•­1", "ì²­êµ¬í•­"],
    "ë„ë©´": ["ë„ë©´1", "ë„ë©´ 1", "ë„1", "ë„ë©´"]
}

def extract_text_from_pdf(file_bytes: bytes) -> str:
    """PDF ë°”ì´íŠ¸ ë°ì´í„°ì—ì„œ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    text = ""
    with pdfplumber.open(io.BytesIO(file_bytes)) as pdf:
        for page in pdf.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
    return text

def detect_format_errors(text: str, diagram_filenames=None) -> List[str]:
    """í…ìŠ¤íŠ¸ì—ì„œ ê·œì¹™ ê¸°ë°˜ í˜•ì‹ ì˜¤ë¥˜ë¥¼ íƒì§€í•©ë‹ˆë‹¤."""
    errors = []
    claim_lines = re.findall(r"^ì²­êµ¬í•­\s*(\d+)\.", text, re.MULTILINE)
    claim_nums_int = list(map(int, claim_lines))

    if len(set(claim_nums_int)) < len(claim_nums_int):
        errors.append("â— ì²­êµ¬í•­ ë²ˆí˜¸ê°€ ì¤‘ë³µë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    if claim_nums_int:
        expected = set(range(min(claim_nums_int), max(claim_nums_int) + 1))
        actual = set(claim_nums_int)
        missing = sorted(expected - actual)
        if missing:
            errors.append(f"â— ì²­êµ¬í•­ ë²ˆí˜¸ {missing} ì´ ëˆ„ë½ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")

    mentioned = set(re.findall(r"ë„\s*(\d+)", text))
    mentioned = set(f"ë„{d}" for d in mentioned)
    if diagram_filenames:
        cleaned = set(f.replace(".png", "").replace(".jpg", "") for f in diagram_filenames)
        missing_diagrams = mentioned - cleaned
        if missing_diagrams:
            errors.append(f"â— ëª…ì„¸ì„œì— ì–¸ê¸‰ë˜ì—ˆìœ¼ë‚˜ ì‹¤ì œ ë„ë©´ì´ ì—†ëŠ” í•­ëª©: {sorted(missing_diagrams)}")

    must_have = ["ë°œëª…ì˜ ëª…ì¹­", "ë„ë©´ì˜ ê°„ë‹¨í•œ ì„¤ëª…", "ë°œëª…ì˜ ìƒì„¸í•œ ì„¤ëª…"]
    for section in must_have:
        if section not in text:
            errors.append(f"â— í•„ìˆ˜ í•­ëª© '{section}' ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return errors

def detect_missing_sections(text: str) -> List[str]:
    """í…ìŠ¤íŠ¸ì—ì„œ í•„ìˆ˜ ì„¹ì…˜ ì „ì²´ê°€ ëˆ„ë½ë˜ì—ˆëŠ”ì§€ íƒì§€í•©ë‹ˆë‹¤."""
    missing_sections = []
    for section, keywords in REQUIRED_SECTIONS.items():
        if not any(keyword in text for keyword in keywords):
            missing_sections.append(section)
    return missing_sections

def extract_claims(text: str) -> List[str]:
    """í…ìŠ¤íŠ¸ì—ì„œ ê°œë³„ ì²­êµ¬í•­ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    pattern = r"(ì²­êµ¬í•­\s*\d+[^\n]*\n.*?)(?=(ì²­êµ¬í•­\s*\d+|ë°œëª…ì˜\s*ì„¤ëª…|ê¸°\s*ìˆ \s*ë¶„\s*ì•¼|$))"
    matches = re.findall(pattern, text, re.DOTALL)
    claims = [m[0].strip() for m in matches]
    return [c for c in claims if len(c) >= 8]

def detect_contextual_errors_with_gpt(paragraph: str, max_chunk_tokens=10000, stride_sentences=2) -> str:
    """GPTë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ë‹¨ì˜ ë¬¸ë§¥ ì˜¤ë¥˜ë¥¼ íƒì§€í•©ë‹ˆë‹¤."""
    tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")
    sentences = re.split(r'(?<=[.ë‹¤!?)])\s+', paragraph)

    chunks = []
    current_chunk = []
    current_token_len = 0
    for sent in sentences:
        token_len = len(tokenizer.encode(sent))
        if current_token_len + token_len > max_chunk_tokens:
            chunks.append(" ".join(current_chunk))
            current_chunk = current_chunk[-stride_sentences:] if stride_sentences else []
            current_token_len = sum(len(tokenizer.encode(s)) for s in current_chunk)
        current_chunk.append(sent)
        current_token_len += token_len
    if current_chunk:
        chunks.append(" ".join(current_chunk))

    def query_gpt(text_chunk):
        prompt = f"""
ë‹¹ì‹ ì€ íŠ¹í—ˆì²­ì—ì„œ ê·¼ë¬´í•˜ëŠ” ìˆ™ë ¨ëœ íŠ¹í—ˆ ì‹¬ì‚¬ê´€ì…ë‹ˆë‹¤.
ì•„ë˜ì˜ ë¬¸ë‹¨ì€ íŠ¹í—ˆ ëª…ì„¸ì„œ(ì²­êµ¬í•­ ë˜ëŠ” ì‹¤ì‹œì˜ˆ) ì¤‘ ì¼ë¶€ì…ë‹ˆë‹¤.

ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ë‹¤ìŒ ê¸°ì¤€ì— ë”°ë¼ **ë¬¸ë§¥ ì˜¤ë¥˜, ìš©ì–´ ë¶ˆì¼ì¹˜, ë…¼ë¦¬ì  ë¹„ì•½ ë˜ëŠ” ì„¤ëª… ë¶€ì¡± ì—¬ë¶€ë¥¼ ì •ë°€í•˜ê²Œ íŒë‹¨**í•˜ê³ , ê° í•­ëª©ì— ëŒ€í•´ ì›ì¸ê³¼ ìˆ˜ì • ì œì•ˆì„ ì œì‹œí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

[ê²€í†  ê¸°ì¤€]
1. ë¬¸ì¥ì˜ ë…¼ë¦¬ì  ì—°ê²°ì´ ìì—°ìŠ¤ëŸ¬ìš´ê°€? (ì›ì¸ â†’ ê²°ê³¼, êµ¬ì„±ìš”ì†Œ â†’ ë™ì‘ ë“±)
2. ë™ì¼ ìš©ì–´ê°€ ë¬¸ë§¥ë§ˆë‹¤ ë‹¤ë¥´ê²Œ ì“°ì´ì§„ ì•Šì•˜ëŠ”ê°€?
3. ë°œëª…ì˜ ëª©ì ê³¼ í•´ê²° ìˆ˜ë‹¨ì´ ì¶©ëŒí•˜ê±°ë‚˜ íë¦„ìƒ ë¹„ì•½ì´ ìˆëŠ”ê°€?
4. íŠ¹ì • êµ¬ì„±ìš”ì†Œì˜ ê¸°ëŠ¥ ë˜ëŠ” êµ¬ì¡°ê°€ ë¶ˆëª…í™•í•˜ì§„ ì•Šì€ê°€?
5. íŠ¹ì • ë¬¸ì¥ì´ ì•ë’¤ ë¬¸ì¥ê³¼ ì˜ë¯¸ìƒ ì¶©ëŒí•˜ì§€ëŠ” ì•ŠëŠ”ê°€?

[ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ]
- ë°œê²¬ëœ ë¬¸ì œ:
- ì›ì¸ ì„¤ëª…:
- ìˆ˜ì • ì œì•ˆ:

ë¶„ì„í•  ë¬¸ë‹¨:
\"\"\"{text_chunk}\"\"\"

ìœ„ ë¬¸ë‹¨ì— ëŒ€í•´ ë¬¸ì œì ì„ ëª¨ë‘ ì°¾ì•„ì£¼ì„¸ìš”.
ë¬¸ì œê°€ ì „í˜€ ì—†ë‹¤ë©´ ì˜ ì‘ì„±í–ˆìœ¼ë‹ˆ ì´ìƒ ì—†ë‹¤ê³  ì•Œë ¤ì£¼ì„¸ìš”.
"""
        try:
            res = client.chat.completions.create(
                model="gpt-4.1",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2
            )
            return res.choices[0].message.content.strip()
        except Exception as e:
            print(f"GPT API í˜¸ì¶œ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
            return "GPT ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    results = []
    for i, chunk in enumerate(chunks):
        print(f"   âš™ï¸ ì²­í¬ {i+1}/{len(chunks)} GPT ë¶„ì„ ì²˜ë¦¬ ì¤‘...")
        results.append(query_gpt(chunk))
    return "\n\n".join(results)

# -----------------------------------------------------------------------------
# APIë¥¼ ìœ„í•œ ë©”ì¸ ê¸°ëŠ¥ í•¨ìˆ˜ (êµ¬ì¡° ê°œì„ )
# -----------------------------------------------------------------------------

def parse_pdf_to_json(file_bytes: bytes) -> Dict:
    """(ê¸°ëŠ¥ 1) PDFë¥¼ íŒŒì‹±í•˜ì—¬ êµ¬ì¡°í™”ëœ JSONìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    text = extract_text_from_pdf(file_bytes)
    
    def find_section(keywords, content, end_keywords):
        end_pattern = '|'.join(end_keywords) if end_keywords else '$'
        pattern = f"({'|'.join(keywords)})(.*?)(?=({end_pattern}))"
        match = re.search(pattern, content, re.DOTALL)
        return match.group(2).strip() if match else ""

    all_keywords = [kw for kws in REQUIRED_SECTIONS.values() for kw in kws]
    
    invention_content = find_section(REQUIRED_SECTIONS["ë°œëª…ì˜ ë‚´ìš©"], text, REQUIRED_SECTIONS["ë„ë©´ì˜ ê°„ë‹¨í•œ ì„¤ëª…"] + REQUIRED_SECTIONS["ì²­êµ¬í•­"])
    
    parsed_data = {
        "title": find_section(REQUIRED_SECTIONS["ë°œëª…ì˜ ëª…ì¹­"], text, all_keywords),
        "technicalField": find_section(REQUIRED_SECTIONS["ê¸°ìˆ  ë¶„ì•¼"], text, all_keywords),
        "backgroundTechnology": find_section(REQUIRED_SECTIONS["ë°°ê²½ ê¸°ìˆ "], text, all_keywords),
        "inventionDetails": {
            "problemToSolve": find_section(REQUIRED_SECTIONS.get("í•´ê²°í•˜ë ¤ëŠ” ê³¼ì œ", []), invention_content, REQUIRED_SECTIONS.get("ê³¼ì œì˜ í•´ê²° ìˆ˜ë‹¨", []) + REQUIRED_SECTIONS.get("ë°œëª…ì˜ íš¨ê³¼", [])),
            "solution": find_section(REQUIRED_SECTIONS.get("ê³¼ì œì˜ í•´ê²° ìˆ˜ë‹¨", []), invention_content, REQUIRED_SECTIONS.get("ë°œëª…ì˜ íš¨ê³¼", [])),
            "effect": find_section(REQUIRED_SECTIONS.get("ë°œëª…ì˜ íš¨ê³¼", []), invention_content, []),
        },
        "summary": "", # ìš”ì•½ì€ ë³„ë„ ë¡œì§ì´ í•„ìš”í•©ë‹ˆë‹¤.
        "drawingDescription": find_section(REQUIRED_SECTIONS["ë„ë©´ì˜ ê°„ë‹¨í•œ ì„¤ëª…"], text, all_keywords),
        "claims": extract_claims(text),
    }
    return parsed_data

def validate_document_from_json(doc_data: dict) -> Dict:
    """(ê¸°ëŠ¥ 2) JSON í˜•ì‹ì˜ ë¬¸ì„œë¥¼ ë°›ì•„ AIë¡œ ê²€ì¦í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    
    # JSON ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ì¬êµ¬ì„±
    full_text_parts = []
    if doc_data.get('title'): full_text_parts.append(f"ë°œëª…ì˜ ëª…ì¹­\n{doc_data.get('title')}")
    if doc_data.get('technicalField'): full_text_parts.append(f"ê¸°ìˆ  ë¶„ì•¼\n{doc_data.get('technicalField')}")
    if doc_data.get('backgroundTechnology'): full_text_parts.append(f"ë°°ê²½ ê¸°ìˆ \n{doc_data.get('backgroundTechnology')}")
    if doc_data.get('drawingDescription'): full_text_parts.append(f"ë„ë©´ì˜ ê°„ë‹¨í•œ ì„¤ëª…\n{doc_data.get('drawingDescription')}")
    full_text_parts.extend(doc_data.get('claims', []))
    full_text = "\n\n".join(full_text_parts)
    
    format_errors = detect_format_errors(full_text)
    missing_sections = detect_missing_sections(full_text)
    
    contextual_issues = []
    claims = doc_data.get('claims', [])
    for i, claim_text in enumerate(claims):
        if not claim_text.strip(): continue
        
        print(f"ğŸ” ì²­êµ¬í•­ {i+1} GPT ë¶„ì„ ì¤‘...")
        analysis = detect_contextual_errors_with_gpt(claim_text)
        
        issue = {
            "id": f'err_ce_{i+1}', "claim": f"ì²­êµ¬í•­ {i+1}", "claimIndex": i,
            "field": "claims", "analysis": analysis,
        }
        
        suggestion_match = re.search(r"\[?ìˆ˜ì • ì œì•ˆ\]?:\s*(.*)", analysis, re.DOTALL)
        if suggestion_match:
            issue["suggestion"] = suggestion_match.group(1).strip()
        
        contextual_issues.append(issue)
        
    return {
        "formatErrors": [{"id": f"err_fe_{i+1}", "message": msg, "field": "claims"} for i, msg in enumerate(format_errors)],
        "missingSections": [{"id": f"err_ms_{i+1}", "message": msg, "field": "backgroundTechnology"} for i, msg in enumerate(missing_sections)],
        "contextualErrors": contextual_issues
    }


# utils.py

def generate_full_draft_from_title(title: str) -> Dict:
    """GPTë¥¼ ì‚¬ìš©í•˜ì—¬ ì œëª©ë§Œìœ¼ë¡œ ì „ì²´ ë¬¸ì„œ ì´ˆì•ˆì„ ìƒì„±í•©ë‹ˆë‹¤."""
    
    # í”„ë¡¬í”„íŠ¸ë¥¼ ìˆ˜ì •í•˜ì—¬, ì‘ë‹µì´ ë°˜ë“œì‹œ JSON í˜•ì‹ì´ì–´ì•¼ í•¨ì„ ëª…í™•íˆ ì§€ì‹œí•©ë‹ˆë‹¤.
    prompt = f"""
ë‹¹ì‹ ì€ ë² í…Œë‘ íŠ¹í—ˆ ë³€ë¦¬ì‚¬ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ 'ë°œëª…ì˜ ëª…ì¹­'ì„ ë°”íƒ•ìœ¼ë¡œ, íŠ¹í—ˆ ëª…ì„¸ì„œì˜ ë‚˜ë¨¸ì§€ í•­ëª©ë“¤ì— ëŒ€í•œ ìƒì„¸í•œ ì´ˆì•ˆì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
ê° í•­ëª©ì€ ì „ë¬¸ì ì´ê³  ë…¼ë¦¬ì ìœ¼ë¡œ ì‘ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
ë‹¤ë¥¸ ì„¤ëª… ì—†ì´, ë°˜ë“œì‹œ ì•„ë˜ì™€ ê°™ì€ JSON ê°ì²´ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”.

[ë°œëª…ì˜ ëª…ì¹­]
{title}

[ìš”êµ¬ë˜ëŠ” JSON í˜•ì‹]
{{
  "technicalField": "...",
  "backgroundTechnology": "...",
  "inventionDetails": {{
    "problemToSolve": "...",
    "solution": "...",
    "effect": "..."
  }},
  "summary": "...",
  "drawingDescription": "...",
  "claims": ["...", "..."]
}}
"""
    try:
        res = client.chat.completions.create(
            model="gpt-4.1",
            messages=[{"role": "user", "content": prompt}],
            # response_format íŒŒë¼ë¯¸í„°ë¥¼ ì œê±°í•˜ì—¬ í˜¸í™˜ì„±ì„ ë†’ì…ë‹ˆë‹¤.
            temperature=0.5
        )
        import json
        # GPT ì‘ë‹µì´ JSON ë¬¸ìì—´ì´ë¯€ë¡œ, ì´ë¥¼ íŒŒì‹±í•˜ì—¬ íŒŒì´ì¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        generated_content = res.choices[0].message.content
        return json.loads(generated_content)
    except Exception as e:
        print(f"GPT ì´ˆì•ˆ ìƒì„± ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        raise e
    

# @tool ë°ì½”ë ˆì´í„°ë¥¼ ë¶™ì—¬ ì´ í•¨ìˆ˜ê°€ AI ì—ì´ì „íŠ¸ê°€ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” 'ë„êµ¬'ì„ì„ ëª…ì‹œí•©ë‹ˆë‹¤.
@tool
def specific_section_validation_tool(patent_document: dict, claim_index: int) -> str:
    """
    íŠ¹í—ˆ ë¬¸ì„œì˜ íŠ¹ì • ì²­êµ¬í•­ í•˜ë‚˜ì˜ ë¬¸ì œì (ë¬¸ë§¥ ì˜¤ë¥˜, ìš©ì–´ ë¶ˆì¼ì¹˜, ë…¼ë¦¬ì  ë¹„ì•½ ë“±)ì„ ì‹¬ì¸µì ìœ¼ë¡œ ë¶„ì„í•˜ê³  ì„¤ëª…í•©ë‹ˆë‹¤.
    ì‚¬ìš©ìê°€ 'ì²­êµ¬í•­ 1ë²ˆ ê²€í† í•´ì¤˜', '3ë²ˆ ì²­êµ¬í•­ì— ë¬´ìŠ¨ ë¬¸ì œ ìˆì–´?' ì™€ ê°™ì´ íŠ¹ì • ì²­êµ¬í•­ì„ ì§€ëª©í•˜ì—¬ ë¶„ì„ì´ë‚˜ ì˜¤ë¥˜ í™•ì¸ì„ ìš”ì²­í•  ë•Œ ì‚¬ìš©í•˜ì„¸ìš”.
    'claim_index'ëŠ” ë¶„ì„í•  ì²­êµ¬í•­ì˜ ë²ˆí˜¸ì…ë‹ˆë‹¤. (ì˜ˆ: 1, 2, 3)
    """
    print(f"--- ë„êµ¬ ì‹¤í–‰: specific_section_validation_tool (ì²­êµ¬í•­ ì¸ë±ìŠ¤: {claim_index}) ---")
    try:
        # claim_indexëŠ” 1ë¶€í„° ì‹œì‘í•˜ë¯€ë¡œ, ë¦¬ìŠ¤íŠ¸ ì¸ë±ìŠ¤ë¥¼ ìœ„í•´ 1ì„ ë¹¼ì¤ë‹ˆë‹¤.
        claim_text = patent_document['claims'][claim_index - 1]
    except (IndexError, KeyError, TypeError):
        return f"ì˜¤ë¥˜: {claim_index}ë²ˆ ì²­êµ¬í•­ì„ ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¬¸ì„œì— ì²­êµ¬í•­ì´ ì˜¬ë°”ë¥´ê²Œ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”."
    
    if not claim_text or not claim_text.strip():
        return f"{claim_index}ë²ˆ ì²­êµ¬í•­ì˜ ë‚´ìš©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."

    # ê¸°ì¡´ì— êµ¬í˜„ëœ GPT ê¸°ë°˜ ë¬¸ë§¥ ì˜¤ë¥˜ ë¶„ì„ í•¨ìˆ˜ë¥¼ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.
    return detect_contextual_errors_with_gpt(claim_text)

@tool
def document_validation_tool(patent_document: dict) -> dict:
    """
    í˜„ì¬ ëŒ€í™” ìƒíƒœì— ìˆëŠ” íŠ¹í—ˆ ë¬¸ì„œ ì „ì²´ì˜ ë¬¸ì œì ì„ ì¢…í•©ì ìœ¼ë¡œ ê²€ì¦í•©ë‹ˆë‹¤. 
    ì‚¬ìš©ìê°€ 'ì „ì²´ ê²€í† ', 'ë¬¸ì œì  ì°¾ì•„ì¤˜' ë“± í¬ê´„ì ì¸ ê²€í† ë¥¼ ìš”ì²­í•  ë•Œ ì‚¬ìš©í•˜ì„¸ìš”.
    ì´ ë„êµ¬ëŠ” ë³„ë„ì˜ ì¸ìê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    """
    print("--- ë„êµ¬ ì‹¤í–‰: document_validation_tool ---")
    # utils.pyì— ì´ë¯¸ ìˆëŠ” í•¨ìˆ˜ë¥¼ ì§ì ‘ í˜¸ì¶œí•©ë‹ˆë‹¤.
    return validate_document_from_json(patent_document)

@tool
def text_refinement_tool(patent_document: dict, claim_index: int, instruction: str) -> str:
    """
    íŠ¹í—ˆ ë¬¸ì„œì˜ íŠ¹ì • ì²­êµ¬í•­ì„ ì£¼ì–´ì§„ ì§€ì‹œì‚¬í•­ì— ë§ê²Œ ì¬ì‘ì„±í•©ë‹ˆë‹¤.
    ì‚¬ìš©ìê°€ 'ì²­êµ¬í•­ 1ë²ˆ ìˆ˜ì •í•´ì¤˜' ì™€ ê°™ì´ íŠ¹ì • ë²ˆí˜¸ë¥¼ ì–¸ê¸‰í•˜ë©° ìˆ˜ì •ì„ ìš”ì²­í•  ë•Œ ì‚¬ìš©í•˜ì„¸ìš”.
    'claim_index'ëŠ” ì²­êµ¬í•­ì˜ ë²ˆí˜¸(ì˜ˆ: 1, 2, 3)ì…ë‹ˆë‹¤.
    """
    print(f"--- ë„êµ¬ ì‹¤í–‰: text_refinement_tool (ì²­êµ¬í•­ ì¸ë±ìŠ¤: {claim_index}) ---")

    # ë„êµ¬ê°€ ì§ì ‘ ë¬¸ì„œì—ì„œ ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì•„ì˜µë‹ˆë‹¤.
    try:
        # claim_indexëŠ” 1ë¶€í„° ì‹œì‘í•˜ë¯€ë¡œ, ë¦¬ìŠ¤íŠ¸ ì¸ë±ìŠ¤ë¥¼ ìœ„í•´ 1ì„ ë¹¼ì¤ë‹ˆë‹¤.
        original_text = patent_document['claims'][claim_index - 1]
    except (IndexError, KeyError):
        return "ì˜¤ë¥˜: í•´ë‹¹ ë²ˆí˜¸ì˜ ì²­êµ¬í•­ì„ ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    prompt = f"""ë‹¹ì‹ ì€ ìµœê³ ì˜ íŠ¹í—ˆ ë³€ë¦¬ì‚¬ì…ë‹ˆë‹¤. ì•„ë˜ì˜ ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ë‹¤ìŒ ì§€ì‹œì‚¬í•­ì— ë§ê²Œ ìˆ˜ì •í•˜ì—¬, ìˆ˜ì •ëœ 'ì²­êµ¬í•­ ì „ë¬¸'ë§Œ ê°„ê²°í•˜ê²Œ ë°˜í™˜í•´ì£¼ì„¸ìš”.
    
    [ì§€ì‹œì‚¬í•­]: {instruction}
    
    [ì›ë³¸ í…ìŠ¤íŠ¸]:
    {original_text}
    
    [ìˆ˜ì •ëœ ì²­êµ¬í•­ ì „ë¬¸]:
    """
    
    response = client.chat.completions.create(
        model="gpt-4.1", # gpt-4.1 ë³´ë‹¤ ìµœì‹ /ìƒìœ„ ëª¨ë¸ ì‚¬ìš© ê¶Œì¥
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3,
    )
    return response.choices[0].message.content.strip()

tools = [document_validation_tool, text_refinement_tool, specific_section_validation_tool]