import io
import re
import pdfplumber
from transformers import GPT2TokenizerFast
import openai 

client = openai.OpenAI(api_key = "")

# ğŸ“‹ í•„ìˆ˜ ì„¹ì…˜ ëˆ„ë½ íƒì§€
REQUIRED_SECTIONS = {
    "ë°œëª…ì˜ ëª…ì¹­": ["ë°œëª…ì˜ ëª…ì¹­", "ë°œëª…ì˜ ì œëª©"],
    "ê¸°ìˆ  ë¶„ì•¼": ["ê¸°ìˆ  ë¶„ì•¼", "ê¸°ìˆ ë¶„ì•¼","ê¸° ìˆ  ë¶„ ì•¼"],
    "ë°°ê²½ ê¸°ìˆ ": ["ë°°ê²½ ê¸°ìˆ ", "ì¢…ë˜ ê¸°ìˆ ", "ì¢…ë˜ê¸°ìˆ ", "ê¸°ì¡´ ê¸°ìˆ ","ë°° ê²½ ê¸° ìˆ "],
    "ë°œëª…ì˜ ë‚´ìš©": ["ë°œëª…ì˜ ë‚´ìš©", "í•´ê²° ìˆ˜ë‹¨", "íš¨ê³¼"],
    "ë„ë©´ì˜ ê°„ë‹¨í•œ ì„¤ëª…": ["ë„ë©´ì˜ ê°„ë‹¨í•œ ì„¤ëª…", "ê°„ë‹¨í•œ ë„ë©´ ì„¤ëª…", "ë„ë©´ ì„¤ëª…"],
    "ë°œëª…ì˜ ì‹¤ì‹œì˜ˆ": ["ì‹¤ì‹œì˜ˆ", "ì‹¤ì‹œ í˜•íƒœ", "ì‹¤ì‹œ ë°©ì‹"],
    "ì²­êµ¬í•­": ["ì²­êµ¬í•­ 1", "ì²­êµ¬í•­1", "ì²­êµ¬í•­"],
    "ë„ë©´": ["ë„ë©´1", "ë„ë©´ 1", "ë„1", "ë„ë©´"]
}

# ğŸ“˜ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜
def extract_text_from_pdf(file_bytes: bytes):
    text = ""
    with pdfplumber.open(io.BytesIO(file_bytes)) as pdf:
        for page in pdf.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
    return text

# ğŸ› ï¸ í˜•ì‹ ì˜¤ë¥˜ íƒì§€
def detect_format_errors(text, diagram_filenames=None):
    errors = []
    claim_lines = re.findall(r"^ì²­êµ¬í•­\s*(\d+)\.", text, re.MULTILINE)
    claim_nums_int = list(map(int, claim_lines))

    if len(set(claim_nums_int)) < len(claim_nums_int):
        errors.append("â— ì²­êµ¬í•­ ë²ˆí˜¸ê°€ ì¤‘ë³µë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    if claim_nums_int:
        expected = set(range(min(claim_nums_int), max(claim_nums_int) + 1))
        actual = set(claim_nums_int)
        missing = sorted(expected - actual)
        if missing:
            errors.append(f"â— ì²­êµ¬í•­ ë²ˆí˜¸ {missing} ì´ ëˆ„ë½ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")

    mentioned = set(re.findall(r"ë„\s*(\d+)", text))
    mentioned = set(f"ë„{d}" for d in mentioned)
    if diagram_filenames:
        cleaned = set(f.replace(".png", "").replace(".jpg", "") for f in diagram_filenames)
        missing_diagrams = mentioned - cleaned
        if missing_diagrams:
            errors.append(f"â— ëª…ì„¸ì„œì— ì–¸ê¸‰ë˜ì—ˆìœ¼ë‚˜ ì‹¤ì œ ë„ë©´ì´ ì—†ëŠ” í•­ëª©: {sorted(missing_diagrams)}")

    must_have = ["ë°œëª…ì˜ ëª…ì¹­", "ë„ë©´ì˜ ê°„ë‹¨í•œ ì„¤ëª…", "ë°œëª…ì˜ ìƒì„¸í•œ ì„¤ëª…"]
    for section in must_have:
        if section not in text:
            errors.append(f"â— í•„ìˆ˜ í•­ëª© '{section}' ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return errors


# ğŸ“‹ í•„ìˆ˜ ì„¹ì…˜ ëˆ„ë½ íƒì§€
def detect_missing_sections(text):
    missing_sections = []
    for section, keywords in REQUIRED_SECTIONS.items():
        if not any(keyword in text for keyword in keywords):
            missing_sections.append(section)
    return missing_sections

# ğŸ” ì²­êµ¬í•­ ì¶”ì¶œ
def extract_claims(text):
    # "ë°œëª…ì˜ ì„¤ëª…" ë˜ëŠ” ë‹¤ìŒ ì²­êµ¬í•­ ë˜ëŠ” ë¬¸ì„œ ëê¹Œì§€ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì²­êµ¬í•­ì„ ì¶”ì¶œ
    pattern = r"(ì²­êµ¬í•­\s*\d+[^\n]*\n.*?)(?=(ì²­êµ¬í•­\s*\d+|ë°œëª…ì˜\s*ì„¤ëª…|ê¸°\s*ìˆ \s*ë¶„\s*ì•¼|$))"
    matches = re.findall(pattern, text, re.DOTALL)

    claims = [m[0].strip() for m in matches]
    claims = [c for c in claims if len(c) >= 8]
    return claims

# âœ‚ï¸ ë¬¸ë§¥ ì˜¤ë¥˜ íƒì§€ (ë¬¸ì¥ ë‹¨ìœ„ ë¶„í•  with ìŠ¬ë¼ì´ë”© ìœˆë„ìš°)
def detect_contextual_errors_with_gpt(paragraph, max_chunk_tokens=10000, stride_sentences=2):
    tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")
    sentences = re.split(r'(?<=[.ë‹¤!?)])\s+', paragraph)

    chunks = []
    current_chunk = []
    current_token_len = 0

    for i, sent in enumerate(sentences):
        token_len = len(tokenizer.encode(sent))
        if current_token_len + token_len > max_chunk_tokens:
            chunks.append(" ".join(current_chunk))
            # ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ì• ë¬¸ì¥ ì¼ë¶€ ìœ ì§€
            current_chunk = current_chunk[-stride_sentences:] if stride_sentences else []
            current_token_len = sum(len(tokenizer.encode(s)) for s in current_chunk)
        current_chunk.append(sent)
        current_token_len += token_len

    if current_chunk:
        chunks.append(" ".join(current_chunk))

    def query_gpt(text_chunk):
        prompt = f"""
ë‹¹ì‹ ì€ íŠ¹í—ˆì²­ì—ì„œ ê·¼ë¬´í•˜ëŠ” ìˆ™ë ¨ëœ íŠ¹í—ˆ ì‹¬ì‚¬ê´€ì…ë‹ˆë‹¤.
ì•„ë˜ì˜ ë¬¸ë‹¨ì€ íŠ¹í—ˆ ëª…ì„¸ì„œ(ì²­êµ¬í•­ ë˜ëŠ” ì‹¤ì‹œì˜ˆ) ì¤‘ ì¼ë¶€ì…ë‹ˆë‹¤.

ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ë‹¤ìŒ ê¸°ì¤€ì— ë”°ë¼ **ë¬¸ë§¥ ì˜¤ë¥˜, ìš©ì–´ ë¶ˆì¼ì¹˜, ë…¼ë¦¬ì  ë¹„ì•½ ë˜ëŠ” ì„¤ëª… ë¶€ì¡± ì—¬ë¶€ë¥¼ ì •ë°€í•˜ê²Œ íŒë‹¨**í•˜ê³ , ê° í•­ëª©ì— ëŒ€í•´ ì›ì¸ê³¼ ìˆ˜ì • ì œì•ˆì„ ì œì‹œí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

[ê²€í†  ê¸°ì¤€]
1. ë¬¸ì¥ì˜ ë…¼ë¦¬ì  ì—°ê²°ì´ ìì—°ìŠ¤ëŸ¬ìš´ê°€? (ì›ì¸ â†’ ê²°ê³¼, êµ¬ì„±ìš”ì†Œ â†’ ë™ì‘ ë“±)
2. ë™ì¼ ìš©ì–´ê°€ ë¬¸ë§¥ë§ˆë‹¤ ë‹¤ë¥´ê²Œ ì“°ì´ì§„ ì•Šì•˜ëŠ”ê°€?
3. ë°œëª…ì˜ ëª©ì ê³¼ í•´ê²° ìˆ˜ë‹¨ì´ ì¶©ëŒí•˜ê±°ë‚˜ íë¦„ìƒ ë¹„ì•½ì´ ìˆëŠ”ê°€?
4. íŠ¹ì • êµ¬ì„±ìš”ì†Œì˜ ê¸°ëŠ¥ ë˜ëŠ” êµ¬ì¡°ê°€ ë¶ˆëª…í™•í•˜ì§„ ì•Šì€ê°€?
5. íŠ¹ì • ë¬¸ì¥ì´ ì•ë’¤ ë¬¸ì¥ê³¼ ì˜ë¯¸ìƒ ì¶©ëŒí•˜ì§€ëŠ” ì•ŠëŠ”ê°€?

[ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ]
- ë°œê²¬ëœ ë¬¸ì œ:
- ì›ì¸ ì„¤ëª…:
- ìˆ˜ì • ì œì•ˆ:

ë¶„ì„í•  ë¬¸ë‹¨:
\"\"\"{text_chunk}\"\"\"

ìœ„ ë¬¸ë‹¨ì— ëŒ€í•´ ë¬¸ì œì ì„ ëª¨ë‘ ì°¾ì•„ì£¼ì„¸ìš”.
ë¬¸ì œê°€ ì „í˜€ ì—†ë‹¤ë©´ ì˜ ì‘ì„±í–ˆìœ¼ë‹ˆ ì´ìƒ ì—†ë‹¤ê³  ì•Œë ¤ì£¼ì„¸ìš”.
"""
        res = client.chat.completions.create(
            model="gpt-4.1",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2
        )
        return res.choices[0].message.content.strip()

    results = []
    for i, chunk in enumerate(chunks):
        print(f"  âš™ï¸ {i+1}/{len(chunks)} ì²˜ë¦¬ ì¤‘...")
        results.append(query_gpt(chunk))

    return "\n\n".join(results)



def process_pdf(file_bytes: bytes):
    text = extract_text_from_pdf(file_bytes)
    format_errors = detect_format_errors(text)
    missing_sections = detect_missing_sections(text)
    claims = extract_claims(text)

    contextual_issues = []
    for i, claim in enumerate(claims):
        issue = detect_contextual_errors_with_gpt(claim)
        contextual_issues.append({
            "claim": f"ì²­êµ¬í•­ {i+1}",
            "issue": issue
        })

    rejection_notice = generate_rejection_notice(format_errors, missing_sections, contextual_issues)

    return {
        "format_errors": format_errors,
        "missing_sections": missing_sections,
        "claims_context_issues": contextual_issues,
        "rejection_notice_draft": rejection_notice
    }

def generate_rejection_notice(format_errors, missing_sections, claims_context_issues):
    summary = ""
    if format_errors:
        summary += "ğŸ“Œ í˜•ì‹ ì˜¤ë¥˜:\n" + "\n".join(f"- {e}" for e in format_errors) + "\n\n"
    if missing_sections:
        summary += "ğŸ“Œ í•„ìˆ˜ í•­ëª© ëˆ„ë½:\n" + "\n".join(f"- {s}" for s in missing_sections) + "\n\n"
    if claims_context_issues:
        summary += "ğŸ“Œ ë¬¸ë§¥ ì˜¤ë¥˜:\n"
        for item in claims_context_issues:
            if "ì´ìƒ ì—†" not in item["issue"]:
                summary += f"- {item['claim']}:\n  {item['issue']}\n\n"

    prompt = f"""
ë‹¹ì‹ ì€ íŠ¹í—ˆì²­ ì‹¬ì‚¬ê´€ì…ë‹ˆë‹¤. ì•„ë˜ ì˜¤ë¥˜ ìš”ì•½ì„ ê¸°ë°˜ìœ¼ë¡œ **ê±°ì ˆì´ìœ  í†µì§€ì„œ ì´ˆì•ˆ**ì„ ì‘ì„±í•˜ì„¸ìš”.

[ì˜¤ë¥˜ ìš”ì•½]
{summary}

[ì‘ì„± ì¡°ê±´]
- ë°œëª…ìì—ê²Œ ì „ë‹¬ë  ë¬¸ì–´ì²´ ë¬¸ì„œ
- ê° ì˜¤ë¥˜ì— ëŒ€í•´ 'ì´ìœ ', 'ê´€ë ¨ ì¡°í•­', 'ë³´ì™„ ë°©ë²•' í¬í•¨
- ì „ì²´ì ìœ¼ë¡œ í•˜ë‚˜ì˜ ì„œìˆ ë¬¸ íë¦„ ìœ ì§€
"""
    res = client.chat.completions.create(
        model="gpt-4.1",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3
    )
    return res.choices[0].message.content.strip()

