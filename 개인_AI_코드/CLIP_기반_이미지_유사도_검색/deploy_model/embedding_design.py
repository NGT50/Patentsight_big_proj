import os
import numpy as np
from PIL import Image
import torch
from transformers import CLIPProcessor, CLIPModel

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# CLIP ëª¨ë¸ ë¡œë”©
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# ì´ë¯¸ì§€ í´ë” ë° ì €ì¥ ê²½ë¡œ
# ğŸ“ 1. ì²˜ë¦¬ ëŒ€ìƒ ê²½ë¡œ ì„¤ì •
image_dir = "KD_IMG/20210105/30"

embedding_dir = "embeddings_clip_design"
os.makedirs(embedding_dir, exist_ok=True)

# ì„ë² ë”© ìˆ˜í–‰
for root, dirs, files in os.walk(image_dir):
    app_num_candidates = [part for part in root.split(os.sep) if part.startswith("30")]
    if not app_num_candidates:
        print(f"â— ì¶œì›ë²ˆí˜¸ ì¶”ì¶œ ì‹¤íŒ¨: {root}")
        continue
    app_num = app_num_candidates[-1]

    for file in files:
        if file.lower().endswith((".jpg", ".jpeg", ".png", ".tif", ".tiff")):
            filepath = os.path.join(root, file)
            print(f"ğŸ” ë°œê²¬í•œ íŒŒì¼: {file}")
            try:
                filestem = os.path.splitext(file)[0]
                view_id = filestem.split("-")[-1] if "-" in filestem else filestem
                output_name = f"{app_num}_{view_id}.npy"

                image = Image.open(filepath).convert("RGB")
                inputs = processor(images=image, return_tensors="pt").to(device)
                with torch.no_grad():
                    image_features = model.get_image_features(**inputs)
                    vector = image_features.squeeze().cpu().numpy()
                    vector = vector / np.linalg.norm(vector)

                np.save(os.path.join(embedding_dir, output_name), vector)
                print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_name}")
            except Exception as e:
                print(f"âŒ {file} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
