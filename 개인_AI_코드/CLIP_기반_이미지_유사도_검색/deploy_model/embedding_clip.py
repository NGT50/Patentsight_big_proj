import os
import numpy as np
from PIL import Image
import torch
from transformers import CLIPProcessor, CLIPModel

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# CLIP ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë”©
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# ì´ë¯¸ì§€ í´ë” ë° ì„ë² ë”© ì €ì¥ í´ë”
image_dir = "Design_sample/4.XML/3020180030717/B012/XML/M001"
embedding_dir = "embeddings_clip_design"
os.makedirs(embedding_dir, exist_ok=True)

# ì„ë² ë”© ìˆ˜í–‰
for root, dirs, files in os.walk(image_dir):
    for file in files:
        print(f"ğŸ” ë°œê²¬í•œ íŒŒì¼: {file}")
        if file.lower().endswith((".jpg", ".jpeg", ".png", ".tif", ".tiff")):
            filepath = os.path.join(root, file)
            try:
                # ì¶œì›ë²ˆí˜¸ ì¶”ì¶œ
                #app_num = os.path.splitext(file)[0].split("_")[0]
                app_num = os.path.splitext(file)[0].split("-")[0]

                # ì´ë¯¸ì§€ ë¡œë”© ë° ì „ì²˜ë¦¬
                image = Image.open(filepath).convert("RGB")
                inputs = processor(images=image, return_tensors="pt").to(device)

                # ì„ë² ë”© ì¶”ì¶œ
                with torch.no_grad():
                    image_features = model.get_image_features(**inputs)
                    vector = image_features.squeeze().cpu().numpy()
                    vector = vector / np.linalg.norm(vector)

                # ì €ì¥
                np.save(os.path.join(embedding_dir, f"{app_num}.npy"), vector)
                print(f"âœ… ì €ì¥ ì™„ë£Œ: {app_num}")
            except Exception as e:
                print(f"âŒ {file} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
