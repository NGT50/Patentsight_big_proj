{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfSQ9xP3YFEv"
   },
   "source": [
    "## ë°ì´í„° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UtWnORahX_AJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ì „ì²˜ë¦¬ëœ ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸\n",
    "all_data = []\n",
    "\n",
    "# ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì„¤ì •\n",
    "root_dir = \"/content/drive/MyDrive/kt_bigproject/data/train\"\n",
    "\n",
    "# ëª¨ë“  JSON íŒŒì¼ ìˆ˜ì§‘\n",
    "json_files = []\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\".json\"):\n",
    "            json_files.append(os.path.join(dirpath, filename))\n",
    "\n",
    "# ì „ì²˜ë¦¬ ìˆ˜í–‰\n",
    "for full_path in tqdm(json_files, desc=\"ğŸ“‚ JSON ì²˜ë¦¬ ì¤‘\"):\n",
    "    try:\n",
    "        with open(full_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw = json.load(f)\n",
    "            dataset = raw.get(\"dataset\", [])\n",
    "            for entry in dataset:\n",
    "                claims = entry.get(\"claims\", \"\").strip()\n",
    "                ipc = entry.get(\"ipc_main\", \"\").strip()\n",
    "                title = entry.get(\"invention_title\", \"\").strip()\n",
    "                year = entry.get(\"application_year\", \"\").strip()\n",
    "\n",
    "                # í•„í„° ì¡°ê±´: claims 30ì ì´ìƒ, ipcì™€ title ì¡´ì¬, ì—°ë„ëŠ” 2000 ì´ìƒ\n",
    "                if claims and ipc and title and year.isdigit() and int(year) >= 2000 and len(claims) > 30:\n",
    "                    all_data.append({\n",
    "                        \"claims\": claims,\n",
    "                        \"ipc_main\": ipc,\n",
    "                        \"title\": title\n",
    "                    })\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {full_path} - {e}\")\n",
    "\n",
    "# ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° ì €ì¥\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(\"ipc_dataset_cleaned.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"\\nâœ… ì „ì²˜ë¦¬ ì™„ë£Œ! ì´ ìƒ˜í”Œ ìˆ˜: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2DYEqsS8Hep"
   },
   "source": [
    "### í´ë˜ìŠ¤ë³„ ìµœì†Œ 10ê°œ ìµœëŒ€ 30ê°œ í•œì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1nHNw9oPYGRC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# ====== 1. ì„¤ì • ======\n",
    "root_dir = \"/content/drive/MyDrive/kt_bigproject/data/train\"\n",
    "save_path = \"/content/drive/MyDrive/kt_bigproject/data/ipc_balanced_by_main_7_28_300.csv\"\n",
    "min_samples = 300\n",
    "max_samples = 301\n",
    "\n",
    "# ====== 2. ëª¨ë“  JSON íŒŒì¼ ê²½ë¡œ ìˆ˜ì§‘ ======\n",
    "json_files = []\n",
    "for dirpath, _, filenames in os.walk(root_dir):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\".json\"):\n",
    "            json_files.append(os.path.join(dirpath, filename))\n",
    "\n",
    "# ====== 3. ipc_mainë³„ë¡œ ë°ì´í„° ìˆ˜ì§‘ ======\n",
    "ipc_main_to_samples = defaultdict(list)\n",
    "\n",
    "for json_path in tqdm(json_files, desc=\"ğŸ“‚ JSON íŒŒì¼ ì²˜ë¦¬ ì¤‘\"):\n",
    "    try:\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            dataset = data.get(\"dataset\", [])\n",
    "            for item in dataset:\n",
    "                claims = item.get(\"claims\", \"\").strip()\n",
    "                ipc_main = item.get(\"ipc_main\", \"\").strip()\n",
    "                ipc_subclass = item.get(\"ipc_subclass\", \"\").strip()\n",
    "                year = item.get(\"application_year\", \"\")\n",
    "\n",
    "                if claims and ipc_main and ipc_subclass and len(claims) > 30:\n",
    "                    try:\n",
    "                        year = int(year)\n",
    "                    except:\n",
    "                        continue  # ì—°ë„ íŒŒì‹± ì•ˆë˜ë©´ ì œì™¸\n",
    "\n",
    "                    ipc_main_to_samples[ipc_main].append({\n",
    "                        \"claims\": claims,\n",
    "                        \"ipc_main\": ipc_main,\n",
    "                        \"ipc_subclass\": ipc_subclass,\n",
    "                        \"application_year\": year\n",
    "                    })\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ - {json_path}: {e}\")\n",
    "\n",
    "# ====== 4. ipc_mainë³„ë¡œ ìµœì‹ ìˆœ ì •ë ¬ í›„ ìƒ˜í”Œ ì¶”ì¶œ ======\n",
    "final_samples = []\n",
    "for ipc_main, samples in tqdm(ipc_main_to_samples.items(), desc=\"ğŸ“¦ ìµœì‹ ìˆœ ìƒ˜í”Œ ì„ íƒ ì¤‘\"):\n",
    "    if len(samples) >= min_samples:\n",
    "        sorted_samples = sorted(samples, key=lambda x: x[\"application_year\"], reverse=True)\n",
    "        selected = sorted_samples[:max_samples]\n",
    "        final_samples.extend(selected)\n",
    "\n",
    "# ====== 5. ì €ì¥ ======\n",
    "df = pd.DataFrame(final_samples)\n",
    "df.to_csv(save_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"\\nâœ… ì „ì²˜ë¦¬ ì™„ë£Œ! ì´ ìƒ˜í”Œ ìˆ˜: {len(df)}, IPC_main ìˆ˜: {df['ipc_main'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9akTrO2YF8E"
   },
   "source": [
    "# ëª¨ë¸ë§"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mh1awGR2gYXS"
   },
   "source": [
    "## GPT ë©”ì¸ í´ë˜ìŠ¤ 1ë‹¨ê³„ ë¶„ë¥˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bw0ihj4Khg6n"
   },
   "source": [
    "### gptíŒŒì¸íŠœë‹ ì—…ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sz1KlL0sYFr4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV íŒŒì¼ ê²½ë¡œ (Colabì— ì—…ë¡œë“œí•œ í›„ ê²½ë¡œ ì§€ì •)\n",
    "file_path = \"/content/drive/MyDrive/kt_bigproject/data/hierarchical_ipc_dataset.csv\"  # â† ë³¸ì¸ íŒŒì¼ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì •\n",
    "\n",
    "# CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_csv(file_path)\n",
    "print(len(df))\n",
    "df['ipc_subclass'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QrSFLCEnYFk1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "file_path = \"/content/drive/MyDrive/kt_bigproject/data/hierarchical_ipc_dataset.csv\"  # â† ë³¸ì¸ íŒŒì¼ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì •\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 2. ipc_subclassë³„ ìµœëŒ€ 5ê°œ ìƒ˜í”Œë§\n",
    "#    (í´ë˜ìŠ¤ë³„ë¡œ ìƒ˜í”Œ ìˆ˜ê°€ 5ê°œë³´ë‹¤ ì ìœ¼ë©´ ìˆëŠ” ë§Œí¼ ìœ ì§€)\n",
    "df_filtered = df.groupby(\"ipc_subclass\", group_keys=False).apply(\n",
    "    lambda x: x.sample(min(len(x), 5), random_state=42)\n",
    ")\n",
    "\n",
    "# 3. ê²°ê³¼ ì €ì¥ (ì„ íƒ)\n",
    "df_filtered.to_csv(\"ipc_subclass_sampled.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ! ì¶”ì¶œëœ í–‰ ìˆ˜: {len(df_filtered)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aG590u_-je4o"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "file_path = \"/content/drive/MyDrive/kt_bigproject/data/ipc_subclass_sampled.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "def convert_to_jsonl(row):\n",
    "    prompt = f\"ë‹¤ìŒ ì²­êµ¬í•­ì— í•´ë‹¹í•˜ëŠ” IPC ì„œë¸Œí´ë˜ìŠ¤ëŠ”?\\n{row['claims']}\"\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "            {\"role\": \"assistant\", \"content\": row['ipc_subclass']}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "jsonl_data = df.progress_apply(convert_to_jsonl, axis=1).tolist()\n",
    "\n",
    "output_path = \"ipc_subclass_finetune_data.jsonl\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in jsonl_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"âœ… JSONL íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U451YoACje1N"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# ìƒˆ í´ë¼ì´ì–¸íŠ¸ ê°ì²´ ìƒì„±\n",
    "client = OpenAI(api_key=\"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\")  # ë³¸ì¸ì˜ API í‚¤ ì…ë ¥\n",
    "\n",
    "# JSONL íŒŒì¼ ê²½ë¡œ\n",
    "jsonl_path = \"/content/ipc_subclass_finetune_data.jsonl\"  # â† Colab ê²½ë¡œì— ë§ê²Œ ìˆ˜ì •\n",
    "\n",
    "# íŒŒì¼ ì—…ë¡œë“œ\n",
    "file_response = client.files.create(\n",
    "    file=open(jsonl_path, \"rb\"),\n",
    "    purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "# íŒŒì¼ ID í™•ì¸\n",
    "print(\"ğŸ“‚ ì—…ë¡œë“œ ì„±ê³µ! File ID:\", file_response.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tfBaE5RkVkq"
   },
   "source": [
    "### main ë¶„ë¥˜ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZqIm0uzjeyY"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.OpenAI(api_key=\"open ai apií‚¤ ì…ë ¥\")  # â† ë„ˆì˜ API í‚¤\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"ft:gpt-3.5-turbo-0125:personal::Bwjsx7ZE\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"ì²­êµ¬í•­ì„ ë³´ê³  IPC subclassë¥¼ ì˜ˆì¸¡í•˜ì„¸ìš”.\"},\n",
    "        {\"role\": \"user\", \"content\": \"ë³¸ ë°œëª…ì€ í† ì–‘ì˜ ìˆ˜ë¶„ì„ íš¨ê³¼ì ìœ¼ë¡œ ìœ ì§€ì‹œí‚¤ê¸° ìœ„í•œ ë†ì—…ìš© í•„ë¦„ì— ê´€í•œ ê²ƒì´ë‹¤.\"}\n",
    "    ],\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "print(\"GPT ì˜ˆì¸¡ subclass:\", response.choices[0].message.content.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXqP0s21j0EF"
   },
   "source": [
    "## SBERT ì„œë¸Œ í´ë˜ìŠ¤ 2ë‹¨ê³„ ë¶„ë¥˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uv3SAZJF1GO"
   },
   "source": [
    "### 2ì°¨ ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sRIazWd_jewC"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# âœ… 1. íŒŒì¼ ê²½ë¡œ ì§€ì • (ì˜ˆ: Google Drive ë˜ëŠ” ì—…ë¡œë“œí•œ íŒŒì¼ ê²½ë¡œ)\n",
    "file_path = \"/content/drive/MyDrive/kt_bigproject/data/ipc/IPC_CODE.txt\"  # â† í•„ìš” ì‹œ ìˆ˜ì •\n",
    "\n",
    "# âœ… 2. íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° (Â¶ë¡œ êµ¬ë¶„)\n",
    "df = pd.read_csv(\n",
    "    file_path,\n",
    "    sep=\"Â¶\",\n",
    "    names=[\"ipc\", \"date\", \"desc_kr\", \"desc_en\"],\n",
    "    engine='python',\n",
    "    quoting=3,\n",
    "    on_bad_lines='skip',\n",
    "    encoding='utf-8'\n",
    ")\n",
    "\n",
    "# âœ… 3. ë‚ ì§œ ë³€í™˜\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "\n",
    "# âœ… 4. ì²« ì¤„(í—¤ë” í–‰) ì œê±°\n",
    "df = df[df[\"ipc\"] != \"IPCì½”ë“œ\"]\n",
    "\n",
    "# âœ… 5. desc_krì´ '.' ë˜ëŠ” ë¹ˆ ë¬¸ìì—´ ë˜ëŠ” NaNì¸ ê²½ìš° ì œê±°\n",
    "df = df[(df[\"desc_kr\"].notna()) & (df[\"desc_kr\"].str.strip() != \".\")]\n",
    "\n",
    "# âœ… 6. ìµœì‹  ê°œì •ì¼ ê¸°ì¤€ IPCë³„ í•˜ë‚˜ë§Œ ë‚¨ê¸°ê¸° (ê¸°ë³¸ ì •ë ¬ì´ ì˜¤ë˜ëœ ìˆœì´ë¼ê³  ê°€ì •)\n",
    "df = df.drop_duplicates(subset=\"ipc\", keep=\"last\")\n",
    "\n",
    "# âœ… 7. desc_en ì»¬ëŸ¼ ì œê±°\n",
    "df = df.drop(columns=[\"desc_en\"]).reset_index(drop=True)\n",
    "\n",
    "# âœ… 8. ì €ì¥\n",
    "save_path = \"/content/drive/MyDrive/kt_bigproject/data/ipc/ipc_kr_latest_cleaned.csv\"\n",
    "df.to_csv(save_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"âœ… ì €ì¥ ì™„ë£Œ: {save_path}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8vhSZ2UGodI"
   },
   "source": [
    "### í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYq35INxF7Tw"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.OpenAI(api_key=\"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\")  # â† ë„ˆì˜ API í‚¤\n",
    "\n",
    "claim = \"í™ë§‰ì´ ëŒ€ìƒ ì˜ì—­ì— ë°°ì¹˜ë˜ëŠ” ì œ1 í”„ë ˆì„;ìƒê¸° ì œ1 í”„ë ˆì„ê³¼ ì´ê²© ë°°ì¹˜ë˜ë„ë¡ ìƒê¸° í™ë§‰ì´ ëŒ€ìƒ ì˜ì—­ì— ë°°ì¹˜ë˜ëŠ” ì œ2 í”„ë ˆì„;ì–‘ë‹¨ë¶€ê°€ ìƒê¸° ì œ1 í”„ë ˆì„ê³¼ ìƒê¸° ì œ2 í”„ë ˆì„ì— ë§ˆë ¨ëœ ì‚½ì…í™ˆì— ê°ê° ê²°í•©ë˜ì–´ ìƒê¸° í™ë§‰ì´ ëŒ€ìƒ ì˜ì—­ì— ìˆëŠ” í™ì„ ë§‰ëŠ” ë³µìˆ˜ì˜ í™ë§‰ì´ ì§€ì§€ë¶€ì¬;ìƒê¸° ì œ1 í”„ë ˆì„ê³¼ ìƒê¸° ì œ2 í”„ë ˆì„ì— ì–‘ë‹¨ë¶€ê°€ ê²°í•© ë˜ì–´ ìƒê¸° ë³µìˆ˜ì˜ í™ë§‰ì´ ì§€ì§€ë¶€ì¬ë¥¼ ì§€ì§€í•˜ëŠ” ì§€ì§€ í”Œë ˆì´íŠ¸; ë°ì¼ì¸¡ë¶€ëŠ” ìƒê¸° í™ì— ë°°ì¹˜ë˜ê³  íƒ€ì¸¡ë¶€ëŠ” ìƒê¸° ì§€ì§€ í”Œë ˆì´íŠ¸ì™€ ìƒê¸° ë³µìˆ˜ì˜ í™ë§‰ì´ ì§€ì§€ë¶€ì¬ì˜ ì¼ë¶€ë¥¼ ê´€í†µí•˜ì—¬ ìƒê¸° ë³µìˆ˜ì˜ í™ë§‰ì´ ì§€ì§€ë¶€ì¬ì˜ ì „ë°©ìœ¼ë¡œ ë°°ì¶œë˜ì–´ ìƒê¸° í™ë§‰ì´ ëŒ€ìƒ ì˜ì—­ì— ìˆëŠ” ë¬¼ì„ ìƒê¸° ë³µìˆ˜ì˜ í™ë§‰ì´ ì§€ì§€ë¶€ì¬ì˜ ì „ë°©ìœ¼ë¡œ ë°°ì¶œì‹œí‚¤ëŠ” ë°°ìˆ˜ë¶€ë¥¼ í¬í•¨í•˜ê³ ,ìƒê¸° ë°°ìˆ˜ë¶€ëŠ”, ì¼ë‹¨ë¶€ê°€ ìƒê¸° ë³µìˆ˜ì˜ í™ë§‰ì´ ì§€ì§€ë¶€ì¬ì˜ ì „ë°©ë¶€ë¡œ ë°°ì¶œë˜ëŠ” ë°°ìˆ˜ íŒŒì´í”„ ë° ì¼ì¸¡ë¶€ì— ìƒê¸° ë°°ìˆ˜ íŒŒì´í”„ê°€ ì—°ê²°ë˜ê³  íƒ€ì¸¡ë¶€ëŠ” ìƒê¸° ì§€ì§€ í”Œë ˆì´íŠ¸ì˜ í›„ë°©ë²½ì— ì§€ì§€ë˜ì–´ ìƒê¸° ë°°ìˆ˜ íŒŒì´í”„ë¡œ ë¬¼ì„ ì•ˆë‚´í•˜ëŠ” ë°°ìˆ˜íŒŒì´í”„ ì§€ì§€íŒì„ í¬í•¨í•˜ê³ ,ìƒê¸° ì§€ì§€ í”Œë ˆì´íŠ¸ëŠ”, ìƒê¸° ë°°ìˆ˜ë¶€ê°€ ê²°í•© ë˜ëŠ” ì˜ì—­ì˜ ìƒê¸° ì§€ì§€ í”Œë ˆì´íŠ¸ì— íƒˆì°© ê°€ëŠ¥í•˜ê²Œ ë§ˆë ¨ë˜ëŠ” ë¶„ë¦¬íŒ ë° ìƒê¸° ë¶„ë¦¬íŒì„ ìƒê¸° ì§€ì§€ í”Œë ˆì´íŠ¸ì— íƒˆì°© ê°€ëŠ¥í•˜ê²Œ ê²°í•©ì‹œí‚¤ëŠ” ë³µìˆ˜ì˜ ë¶„ë¦¬íŒ ê²°í•© ë¶€ì¬ë¥¼ í¬í•¨í•˜ê³ ,ìƒê¸° ë³µìˆ˜ì˜ í™ë§‰ì´ ì§€ì§€ë¶€ì¬ëŠ” ë°” í˜•ìƒì„ ê°€ì§€ê³ ,ìƒê¸° ë³µìˆ˜ì˜ í™ë§‰ì´ ì§€ì§€ë¶€ì¬ì—ëŠ” ìˆ˜ì§ ë°©í–¥ìœ¼ë¡œ ê°ê°ì˜ í™ë§‰ì´ ì§€ì§€ë¶€ì¬ë¥¼ ê´€í†µí•˜ì—¬ ê²°í•©ë˜ëŠ” ê³ ì • ë¶€ì¬ê°€ ë§ˆë ¨ë˜ê³ ,ìƒê¸° ë°°ìˆ˜íŒŒì´í”„ ì§€ì§€íŒì€ ìƒê¸° ë°°ìˆ˜ íŒŒì´í”„ë³´ë‹¤ í¬ê²Œ ë§ˆë ¨ë˜ì–´ ì¼ì¸¡ë©´ì´ ì§€ì§€ í”Œë ˆì´íŠ¸ì— ì§€ì§€ë˜ê³ ,ìƒê¸° ì œ1 í”„ë ˆì„ê³¼ ìƒê¸° ì œ2 í”„ë ˆì„ì— ê°ê° ê²°í•© ë˜ëŠ” ì»¤ë²„ í”„ë ˆì„ì„ ë” í¬í•¨í•˜ê³ ,ìƒê¸° ì œ1 í”„ë ˆì„ê³¼ ìƒê¸° ì œ2 í”„ë ˆì„ì˜ ìƒë‹¨ë¶€ì— ê°ê° ê²°í•© ë˜ëŠ” ìº¡ ë¶€ì¬ë¥¼ ë” í¬í•¨í•˜ê³ ,ìƒê¸° ë°°ìˆ˜ íŒŒì´í”„ëŠ” ìƒê¸° ë¶„ë¦¬íŒì— ê´€í†µ ê²°í•©ë˜ê³ ,ìƒê¸° ë°°ìˆ˜ íŒŒì´í”„(510)ì™€ ì ‘í•˜ëŠ” ì˜ì—­ì˜ ìƒê¸° í™ë§‰ì´ ì§€ì§€ë¶€ì¬(300)ì—ëŠ” ìƒê¸° ë°°ìˆ˜ íŒŒì´í”„(510)ì˜ ì™¸ê²½ì— ëŒ€ì‘ë˜ëŠ” ì ˆê°œí™ˆì´ ë§ˆë ¨ë˜ì–´ ìƒê¸° ë°°ìˆ˜ íŒŒì´í”„(510)ì™€ ìƒê¸° í™ë§‰ì´ ì§€ì§€ë¶€ì¬(300)ë¥¼ ë°€ì°©ì‹œí‚¬ ìˆ˜ ìˆê³ ,ìƒê¸° ë³µìˆ˜ì˜ í™ë§‰ì´ ì§€ì§€ë¶€ì¬(300)ì—ëŠ” ìƒê¸° ë¶„ë¦¬íŒ ê²°í•©ë¶€ì¬(420)ì— ëŒ€ì‘ë˜ëŠ” ìˆ˜ìš©í™ˆì´ ë§ˆë ¨ë˜ì–´ ìƒê¸° ë¶„ë¦¬íŒ ê²°í•©ë¶€ì¬(420)ë¥¼ ìˆ˜ìš©í•  ìˆ˜ ìˆëŠ” ê²ƒì„ íŠ¹ì§•ìœ¼ë¡œ í•˜ëŠ” ì¡°ê²½ìš© ì¶•ëŒ€ëª©. \"\n",
    "response = client.chat.completions.create(\n",
    "    model=\"ft:gpt-3.5-turbo-0125:personal::Bwjsx7ZE\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"ì²­êµ¬í•­ì„ ë³´ê³  IPC subclassë¥¼ ì˜ˆì¸¡í•˜ì„¸ìš”. ê°€ëŠ¥ì„±ì´ ë†’ì€ 3ê°œì˜ subclassë¥¼ ì½¤ë§ˆë¡œ êµ¬ë¶„í•˜ì—¬ ì¶œë ¥í•˜ì„¸ìš”.\"},\n",
    "        {\"role\": \"user\", \"content\": claim}\n",
    "    ],\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "subclass_candidates = response.choices[0].message.content.strip().replace(\" \", \"\").split(\",\")\n",
    "print(\"GPT ì˜ˆì¸¡ subclass í›„ë³´:\", subclass_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhVp7XLsHBbx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/kt_bigproject/data/ipc/ipc_kr_latest_cleaned.csv\")\n",
    "df[\"subclass\"] = df[\"ipc\"].str.extract(r\"^([A-Z]\\d{2}[A-Z]?)\")\n",
    "df[\"maingroup\"] = df[\"ipc\"]\n",
    "\n",
    "# GPTê°€ ì¤€ subclass í›„ë³´ë§Œ í•„í„°ë§\n",
    "filtered_df = df[df[\"subclass\"].isin(subclass_candidates)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ea1HWkFHDav"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer(\"jhgan/ko-sbert-nli\")\n",
    "claim_emb = model.encode(claim, convert_to_tensor=True)\n",
    "\n",
    "# ì„¤ëª… ê°•í™”\n",
    "filtered_df[\"desc_full\"] = filtered_df[\"maingroup\"] + \" \" + filtered_df[\"desc_kr\"]\n",
    "\n",
    "# ì„ë² ë”© + ìœ ì‚¬ë„\n",
    "filtered_df[\"desc_emb\"] = filtered_df[\"desc_full\"].apply(lambda x: model.encode(str(x), convert_to_tensor=True))\n",
    "filtered_df[\"score\"] = filtered_df[\"desc_emb\"].apply(lambda emb: util.cos_sim(claim_emb, emb).item())\n",
    "\n",
    "# ê²°ê³¼ ì •ë ¬\n",
    "top_match = filtered_df.sort_values(\"score\", ascending=False).head(5)\n",
    "print(\"âœ… SBERT ì¶”ì²œ:\")\n",
    "print(top_match[[\"maingroup\", \"desc_kr\", \"score\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7J5Zaw3VHHTy"
   },
   "source": [
    "#### í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lpkp-gLfHG_d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import openai\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "\n",
    "# âœ… 1. íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° (ì •ì œëœ í‰ê°€ì…‹)\n",
    "eval_df = pd.read_csv(\"/content/drive/MyDrive/kt_bigproject/data/ipc_dataset_cleaned_normalized.csv\")\n",
    "\n",
    "# âœ… 2. ì¼ë¶€ ìƒ˜í”Œ ì¶”ì¶œ (ì†ë„ ê³ ë ¤, ì›í•˜ëŠ” ê°œìˆ˜ë§Œ)\n",
    "# í‰ê°€ìš© ìƒ˜í”Œ 10ê°œë§Œ ì¶”ì¶œ (ì»¬ëŸ¼ ì´ë¦„ ë§ê²Œ)\n",
    "sample_eval = eval_df.sample(n=50, random_state=42)[[\"claims\", \"ipc_normalized\"]] \\\n",
    "                     .rename(columns={\"claims\": \"claim\", \"ipc_normalized\": \"label\"}) \\\n",
    "                     .reset_index(drop=True)\n",
    "\n",
    "print(\"ìƒ˜í”Œ ìˆ˜:\", len(sample_eval))\n",
    "sample_eval.head()\n",
    "\n",
    "ipc_df = pd.read_csv(\"/content/drive/MyDrive/kt_bigproject/data/ipc/ipc_kr_latest_cleaned.csv\")\n",
    "ipc_df[\"subclass\"] = ipc_df[\"ipc\"].str.extract(r\"^([A-Z]\\d{2}[A-Z]?)\")\n",
    "ipc_df[\"maingroup\"] = ipc_df[\"ipc\"]\n",
    "\n",
    "# SBERT ëª¨ë¸\n",
    "sbert_model = SentenceTransformer(\"jhgan/ko-sbert-nli\")  # ë˜ëŠ” ë‹¤ë¥¸ SBERT ëª¨ë¸\n",
    "\n",
    "# OpenAI GPT ì„¤ì •\n",
    "import openai\n",
    "openai_client = openai.OpenAI(api_key=\"open ai apií‚¤ ì…ë ¥\")  # ë„ˆì˜ í‚¤ ì…ë ¥\n",
    "gpt_model_name = \"ft:gpt-3.5-turbo-0125:personal::Bwjsx7ZE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eraZdTM1HQ19"
   },
   "outputs": [],
   "source": [
    "def normalize_ipc(code):\n",
    "    if not isinstance(code, str):\n",
    "        return \"\"\n",
    "    code = code.upper().replace(\"-\", \"\")\n",
    "    code = re.sub(r\"([A-Z]\\d{2}[A-Z]?)(0+)(\\d+/)\", r\"\\1\\3\", code)\n",
    "    return code.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0p6EYvfMHQu3"
   },
   "outputs": [],
   "source": [
    "def predict_ipc(claim, client, df, model, gpt_model_name):\n",
    "    # GPTë¡œ subclass top-3 ì˜ˆì¸¡\n",
    "    response = client.chat.completions.create(\n",
    "        model=gpt_model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"ì²­êµ¬í•­ì„ ë³´ê³  IPC subclassë¥¼ ì˜ˆì¸¡í•˜ì„¸ìš”. ê°€ëŠ¥ì„±ì´ ë†’ì€ 3ê°œì˜ subclassë¥¼ ì½¤ë§ˆë¡œ êµ¬ë¶„í•˜ì—¬ ì¶œë ¥í•˜ì„¸ìš”.\"},\n",
    "            {\"role\": \"user\", \"content\": claim}\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    subclass_candidates = response.choices[0].message.content.strip().replace(\" \", \"\").split(\",\")\n",
    "\n",
    "    # í•´ë‹¹ subclass IPCë§Œ í•„í„°ë§\n",
    "    candidates = df[df[\"subclass\"].isin(subclass_candidates)].copy()\n",
    "    candidates[\"desc_full\"] = candidates[\"maingroup\"] + \" \" + candidates[\"desc_kr\"]\n",
    "\n",
    "    claim_emb = model.encode(claim, convert_to_tensor=True)\n",
    "    candidates[\"desc_emb\"] = candidates[\"desc_full\"].apply(lambda x: model.encode(str(x), convert_to_tensor=True))\n",
    "    candidates[\"score\"] = candidates[\"desc_emb\"].apply(lambda emb: util.cos_sim(claim_emb, emb).item())\n",
    "\n",
    "    top1 = candidates.sort_values(\"score\", ascending=False).iloc[0][\"maingroup\"]\n",
    "    return normalize_ipc(top1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4vYIWXMsHU4f"
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "results = []\n",
    "\n",
    "for idx, row in tqdm(sample_eval.iterrows(), total=len(sample_eval)):\n",
    "    claim = row[\"claim\"]\n",
    "    label = normalize_ipc(row[\"label\"])\n",
    "\n",
    "    try:\n",
    "        pred = predict_ipc(claim, openai_client, ipc_df, sbert_model, gpt_model_name)\n",
    "        is_correct = (pred == label)\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "    except Exception as e:\n",
    "        pred = f\"error: {e}\"\n",
    "        is_correct = False\n",
    "\n",
    "    results.append({\n",
    "        \"claim\": claim,\n",
    "        \"label\": label,\n",
    "        \"predicted\": pred,\n",
    "        \"correct\": is_correct\n",
    "    })\n",
    "\n",
    "accuracy = correct / len(sample_eval)\n",
    "print(f\"âœ… Top-1 ì •í™•ë„: {accuracy:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPnpVaOjfIVQRTsMafP1fE1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
