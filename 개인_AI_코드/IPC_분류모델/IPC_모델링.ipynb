{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfSQ9xP3YFEv"
   },
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UtWnORahX_AJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 전처리된 결과 저장 리스트\n",
    "all_data = []\n",
    "\n",
    "# 루트 디렉토리 설정\n",
    "root_dir = \"/content/drive/MyDrive/kt_bigproject/data/train\"\n",
    "\n",
    "# 모든 JSON 파일 수집\n",
    "json_files = []\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\".json\"):\n",
    "            json_files.append(os.path.join(dirpath, filename))\n",
    "\n",
    "# 전처리 수행\n",
    "for full_path in tqdm(json_files, desc=\"📂 JSON 처리 중\"):\n",
    "    try:\n",
    "        with open(full_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw = json.load(f)\n",
    "            dataset = raw.get(\"dataset\", [])\n",
    "            for entry in dataset:\n",
    "                claims = entry.get(\"claims\", \"\").strip()\n",
    "                ipc = entry.get(\"ipc_main\", \"\").strip()\n",
    "                title = entry.get(\"invention_title\", \"\").strip()\n",
    "                year = entry.get(\"application_year\", \"\").strip()\n",
    "\n",
    "                # 필터 조건: claims 30자 이상, ipc와 title 존재, 연도는 2000 이상\n",
    "                if claims and ipc and title and year.isdigit() and int(year) >= 2000 and len(claims) > 30:\n",
    "                    all_data.append({\n",
    "                        \"claims\": claims,\n",
    "                        \"ipc_main\": ipc,\n",
    "                        \"title\": title\n",
    "                    })\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 오류 발생: {full_path} - {e}\")\n",
    "\n",
    "# 데이터프레임 생성 및 저장\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(\"ipc_dataset_cleaned.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"\\n✅ 전처리 완료! 총 샘플 수: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2DYEqsS8Hep"
   },
   "source": [
    "### 클래스별 최소 10개 최대 30개 한정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1nHNw9oPYGRC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# ====== 1. 설정 ======\n",
    "root_dir = \"/content/drive/MyDrive/kt_bigproject/data/train\"\n",
    "save_path = \"/content/drive/MyDrive/kt_bigproject/data/ipc_balanced_by_main_7_28_300.csv\"\n",
    "min_samples = 300\n",
    "max_samples = 301\n",
    "\n",
    "# ====== 2. 모든 JSON 파일 경로 수집 ======\n",
    "json_files = []\n",
    "for dirpath, _, filenames in os.walk(root_dir):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\".json\"):\n",
    "            json_files.append(os.path.join(dirpath, filename))\n",
    "\n",
    "# ====== 3. ipc_main별로 데이터 수집 ======\n",
    "ipc_main_to_samples = defaultdict(list)\n",
    "\n",
    "for json_path in tqdm(json_files, desc=\"📂 JSON 파일 처리 중\"):\n",
    "    try:\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            dataset = data.get(\"dataset\", [])\n",
    "            for item in dataset:\n",
    "                claims = item.get(\"claims\", \"\").strip()\n",
    "                ipc_main = item.get(\"ipc_main\", \"\").strip()\n",
    "                ipc_subclass = item.get(\"ipc_subclass\", \"\").strip()\n",
    "                year = item.get(\"application_year\", \"\")\n",
    "\n",
    "                if claims and ipc_main and ipc_subclass and len(claims) > 30:\n",
    "                    try:\n",
    "                        year = int(year)\n",
    "                    except:\n",
    "                        continue  # 연도 파싱 안되면 제외\n",
    "\n",
    "                    ipc_main_to_samples[ipc_main].append({\n",
    "                        \"claims\": claims,\n",
    "                        \"ipc_main\": ipc_main,\n",
    "                        \"ipc_subclass\": ipc_subclass,\n",
    "                        \"application_year\": year\n",
    "                    })\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 오류 발생 - {json_path}: {e}\")\n",
    "\n",
    "# ====== 4. ipc_main별로 최신순 정렬 후 샘플 추출 ======\n",
    "final_samples = []\n",
    "for ipc_main, samples in tqdm(ipc_main_to_samples.items(), desc=\"📦 최신순 샘플 선택 중\"):\n",
    "    if len(samples) >= min_samples:\n",
    "        sorted_samples = sorted(samples, key=lambda x: x[\"application_year\"], reverse=True)\n",
    "        selected = sorted_samples[:max_samples]\n",
    "        final_samples.extend(selected)\n",
    "\n",
    "# ====== 5. 저장 ======\n",
    "df = pd.DataFrame(final_samples)\n",
    "df.to_csv(save_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"\\n✅ 전처리 완료! 총 샘플 수: {len(df)}, IPC_main 수: {df['ipc_main'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9akTrO2YF8E"
   },
   "source": [
    "# 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mh1awGR2gYXS"
   },
   "source": [
    "## GPT 메인 클래스 1단계 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bw0ihj4Khg6n"
   },
   "source": [
    "### gpt파인튜닝 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sz1KlL0sYFr4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV 파일 경로 (Colab에 업로드한 후 경로 지정)\n",
    "file_path = \"/content/drive/MyDrive/kt_bigproject/data/hierarchical_ipc_dataset.csv\"  # ← 본인 파일 경로에 맞게 수정\n",
    "\n",
    "# CSV 파일 불러오기\n",
    "df = pd.read_csv(file_path)\n",
    "print(len(df))\n",
    "df['ipc_subclass'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QrSFLCEnYFk1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. CSV 파일 불러오기\n",
    "file_path = \"/content/drive/MyDrive/kt_bigproject/data/hierarchical_ipc_dataset.csv\"  # ← 본인 파일 경로에 맞게 수정\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 2. ipc_subclass별 최대 5개 샘플링\n",
    "#    (클래스별로 샘플 수가 5개보다 적으면 있는 만큼 유지)\n",
    "df_filtered = df.groupby(\"ipc_subclass\", group_keys=False).apply(\n",
    "    lambda x: x.sample(min(len(x), 5), random_state=42)\n",
    ")\n",
    "\n",
    "# 3. 결과 저장 (선택)\n",
    "df_filtered.to_csv(\"ipc_subclass_sampled.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"✅ 전처리 완료! 추출된 행 수: {len(df_filtered)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aG590u_-je4o"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "file_path = \"/content/drive/MyDrive/kt_bigproject/data/ipc_subclass_sampled.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "def convert_to_jsonl(row):\n",
    "    prompt = f\"다음 청구항에 해당하는 IPC 서브클래스는?\\n{row['claims']}\"\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "            {\"role\": \"assistant\", \"content\": row['ipc_subclass']}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "jsonl_data = df.progress_apply(convert_to_jsonl, axis=1).tolist()\n",
    "\n",
    "output_path = \"ipc_subclass_finetune_data.jsonl\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in jsonl_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"✅ JSONL 파일 저장 완료: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U451YoACje1N"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# 새 클라이언트 객체 생성\n",
    "client = OpenAI(api_key=\"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\")  # 본인의 API 키 입력\n",
    "\n",
    "# JSONL 파일 경로\n",
    "jsonl_path = \"/content/ipc_subclass_finetune_data.jsonl\"  # ← Colab 경로에 맞게 수정\n",
    "\n",
    "# 파일 업로드\n",
    "file_response = client.files.create(\n",
    "    file=open(jsonl_path, \"rb\"),\n",
    "    purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "# 파일 ID 확인\n",
    "print(\"📂 업로드 성공! File ID:\", file_response.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tfBaE5RkVkq"
   },
   "source": [
    "### main 분류 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZqIm0uzjeyY"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.OpenAI(api_key=\"open ai api키 입력\")  # ← 너의 API 키\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"ft:gpt-3.5-turbo-0125:personal::Bwjsx7ZE\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"청구항을 보고 IPC subclass를 예측하세요.\"},\n",
    "        {\"role\": \"user\", \"content\": \"본 발명은 토양의 수분을 효과적으로 유지시키기 위한 농업용 필름에 관한 것이다.\"}\n",
    "    ],\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "print(\"GPT 예측 subclass:\", response.choices[0].message.content.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXqP0s21j0EF"
   },
   "source": [
    "## SBERT 서브 클래스 2단계 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uv3SAZJF1GO"
   },
   "source": [
    "### 2차 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sRIazWd_jewC"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ✅ 1. 파일 경로 지정 (예: Google Drive 또는 업로드한 파일 경로)\n",
    "file_path = \"/content/drive/MyDrive/kt_bigproject/data/ipc/IPC_CODE.txt\"  # ← 필요 시 수정\n",
    "\n",
    "# ✅ 2. 파일 불러오기 (¶로 구분)\n",
    "df = pd.read_csv(\n",
    "    file_path,\n",
    "    sep=\"¶\",\n",
    "    names=[\"ipc\", \"date\", \"desc_kr\", \"desc_en\"],\n",
    "    engine='python',\n",
    "    quoting=3,\n",
    "    on_bad_lines='skip',\n",
    "    encoding='utf-8'\n",
    ")\n",
    "\n",
    "# ✅ 3. 날짜 변환\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "\n",
    "# ✅ 4. 첫 줄(헤더 행) 제거\n",
    "df = df[df[\"ipc\"] != \"IPC코드\"]\n",
    "\n",
    "# ✅ 5. desc_kr이 '.' 또는 빈 문자열 또는 NaN인 경우 제거\n",
    "df = df[(df[\"desc_kr\"].notna()) & (df[\"desc_kr\"].str.strip() != \".\")]\n",
    "\n",
    "# ✅ 6. 최신 개정일 기준 IPC별 하나만 남기기 (기본 정렬이 오래된 순이라고 가정)\n",
    "df = df.drop_duplicates(subset=\"ipc\", keep=\"last\")\n",
    "\n",
    "# ✅ 7. desc_en 컬럼 제거\n",
    "df = df.drop(columns=[\"desc_en\"]).reset_index(drop=True)\n",
    "\n",
    "# ✅ 8. 저장\n",
    "save_path = \"/content/drive/MyDrive/kt_bigproject/data/ipc/ipc_kr_latest_cleaned.csv\"\n",
    "df.to_csv(save_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"✅ 저장 완료: {save_path}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8vhSZ2UGodI"
   },
   "source": [
    "### 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYq35INxF7Tw"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.OpenAI(api_key=\"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\")  # ← 너의 API 키\n",
    "\n",
    "claim = \"흙막이 대상 영역에 배치되는 제1 프레임;상기 제1 프레임과 이격 배치되도록 상기 흙막이 대상 영역에 배치되는 제2 프레임;양단부가 상기 제1 프레임과 상기 제2 프레임에 마련된 삽입홈에 각각 결합되어 상기 흙막이 대상 영역에 있는 흙을 막는 복수의 흙막이 지지부재;상기 제1 프레임과 상기 제2 프레임에 양단부가 결합 되어 상기 복수의 흙막이 지지부재를 지지하는 지지 플레이트; 및일측부는 상기 흙에 배치되고 타측부는 상기 지지 플레이트와 상기 복수의 흙막이 지지부재의 일부를 관통하여 상기 복수의 흙막이 지지부재의 전방으로 배출되어 상기 흙막이 대상 영역에 있는 물을 상기 복수의 흙막이 지지부재의 전방으로 배출시키는 배수부를 포함하고,상기 배수부는, 일단부가 상기 복수의 흙막이 지지부재의 전방부로 배출되는 배수 파이프 및 일측부에 상기 배수 파이프가 연결되고 타측부는 상기 지지 플레이트의 후방벽에 지지되어 상기 배수 파이프로 물을 안내하는 배수파이프 지지판을 포함하고,상기 지지 플레이트는, 상기 배수부가 결합 되는 영역의 상기 지지 플레이트에 탈착 가능하게 마련되는 분리판 및 상기 분리판을 상기 지지 플레이트에 탈착 가능하게 결합시키는 복수의 분리판 결합 부재를 포함하고,상기 복수의 흙막이 지지부재는 바 형상을 가지고,상기 복수의 흙막이 지지부재에는 수직 방향으로 각각의 흙막이 지지부재를 관통하여 결합되는 고정 부재가 마련되고,상기 배수파이프 지지판은 상기 배수 파이프보다 크게 마련되어 일측면이 지지 플레이트에 지지되고,상기 제1 프레임과 상기 제2 프레임에 각각 결합 되는 커버 프레임을 더 포함하고,상기 제1 프레임과 상기 제2 프레임의 상단부에 각각 결합 되는 캡 부재를 더 포함하고,상기 배수 파이프는 상기 분리판에 관통 결합되고,상기 배수 파이프(510)와 접하는 영역의 상기 흙막이 지지부재(300)에는 상기 배수 파이프(510)의 외경에 대응되는 절개홈이 마련되어 상기 배수 파이프(510)와 상기 흙막이 지지부재(300)를 밀착시킬 수 있고,상기 복수의 흙막이 지지부재(300)에는 상기 분리판 결합부재(420)에 대응되는 수용홈이 마련되어 상기 분리판 결합부재(420)를 수용할 수 있는 것을 특징으로 하는 조경용 축대목. \"\n",
    "response = client.chat.completions.create(\n",
    "    model=\"ft:gpt-3.5-turbo-0125:personal::Bwjsx7ZE\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"청구항을 보고 IPC subclass를 예측하세요. 가능성이 높은 3개의 subclass를 콤마로 구분하여 출력하세요.\"},\n",
    "        {\"role\": \"user\", \"content\": claim}\n",
    "    ],\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "subclass_candidates = response.choices[0].message.content.strip().replace(\" \", \"\").split(\",\")\n",
    "print(\"GPT 예측 subclass 후보:\", subclass_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhVp7XLsHBbx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/kt_bigproject/data/ipc/ipc_kr_latest_cleaned.csv\")\n",
    "df[\"subclass\"] = df[\"ipc\"].str.extract(r\"^([A-Z]\\d{2}[A-Z]?)\")\n",
    "df[\"maingroup\"] = df[\"ipc\"]\n",
    "\n",
    "# GPT가 준 subclass 후보만 필터링\n",
    "filtered_df = df[df[\"subclass\"].isin(subclass_candidates)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ea1HWkFHDav"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer(\"jhgan/ko-sbert-nli\")\n",
    "claim_emb = model.encode(claim, convert_to_tensor=True)\n",
    "\n",
    "# 설명 강화\n",
    "filtered_df[\"desc_full\"] = filtered_df[\"maingroup\"] + \" \" + filtered_df[\"desc_kr\"]\n",
    "\n",
    "# 임베딩 + 유사도\n",
    "filtered_df[\"desc_emb\"] = filtered_df[\"desc_full\"].apply(lambda x: model.encode(str(x), convert_to_tensor=True))\n",
    "filtered_df[\"score\"] = filtered_df[\"desc_emb\"].apply(lambda emb: util.cos_sim(claim_emb, emb).item())\n",
    "\n",
    "# 결과 정렬\n",
    "top_match = filtered_df.sort_values(\"score\", ascending=False).head(5)\n",
    "print(\"✅ SBERT 추천:\")\n",
    "print(top_match[[\"maingroup\", \"desc_kr\", \"score\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7J5Zaw3VHHTy"
   },
   "source": [
    "#### 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lpkp-gLfHG_d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import openai\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "\n",
    "# ✅ 1. 파일 불러오기 (정제된 평가셋)\n",
    "eval_df = pd.read_csv(\"/content/drive/MyDrive/kt_bigproject/data/ipc_dataset_cleaned_normalized.csv\")\n",
    "\n",
    "# ✅ 2. 일부 샘플 추출 (속도 고려, 원하는 개수만)\n",
    "# 평가용 샘플 10개만 추출 (컬럼 이름 맞게)\n",
    "sample_eval = eval_df.sample(n=50, random_state=42)[[\"claims\", \"ipc_normalized\"]] \\\n",
    "                     .rename(columns={\"claims\": \"claim\", \"ipc_normalized\": \"label\"}) \\\n",
    "                     .reset_index(drop=True)\n",
    "\n",
    "print(\"샘플 수:\", len(sample_eval))\n",
    "sample_eval.head()\n",
    "\n",
    "ipc_df = pd.read_csv(\"/content/drive/MyDrive/kt_bigproject/data/ipc/ipc_kr_latest_cleaned.csv\")\n",
    "ipc_df[\"subclass\"] = ipc_df[\"ipc\"].str.extract(r\"^([A-Z]\\d{2}[A-Z]?)\")\n",
    "ipc_df[\"maingroup\"] = ipc_df[\"ipc\"]\n",
    "\n",
    "# SBERT 모델\n",
    "sbert_model = SentenceTransformer(\"jhgan/ko-sbert-nli\")  # 또는 다른 SBERT 모델\n",
    "\n",
    "# OpenAI GPT 설정\n",
    "import openai\n",
    "openai_client = openai.OpenAI(api_key=\"open ai api키 입력\")  # 너의 키 입력\n",
    "gpt_model_name = \"ft:gpt-3.5-turbo-0125:personal::Bwjsx7ZE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eraZdTM1HQ19"
   },
   "outputs": [],
   "source": [
    "def normalize_ipc(code):\n",
    "    if not isinstance(code, str):\n",
    "        return \"\"\n",
    "    code = code.upper().replace(\"-\", \"\")\n",
    "    code = re.sub(r\"([A-Z]\\d{2}[A-Z]?)(0+)(\\d+/)\", r\"\\1\\3\", code)\n",
    "    return code.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0p6EYvfMHQu3"
   },
   "outputs": [],
   "source": [
    "def predict_ipc(claim, client, df, model, gpt_model_name):\n",
    "    # GPT로 subclass top-3 예측\n",
    "    response = client.chat.completions.create(\n",
    "        model=gpt_model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"청구항을 보고 IPC subclass를 예측하세요. 가능성이 높은 3개의 subclass를 콤마로 구분하여 출력하세요.\"},\n",
    "            {\"role\": \"user\", \"content\": claim}\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    subclass_candidates = response.choices[0].message.content.strip().replace(\" \", \"\").split(\",\")\n",
    "\n",
    "    # 해당 subclass IPC만 필터링\n",
    "    candidates = df[df[\"subclass\"].isin(subclass_candidates)].copy()\n",
    "    candidates[\"desc_full\"] = candidates[\"maingroup\"] + \" \" + candidates[\"desc_kr\"]\n",
    "\n",
    "    claim_emb = model.encode(claim, convert_to_tensor=True)\n",
    "    candidates[\"desc_emb\"] = candidates[\"desc_full\"].apply(lambda x: model.encode(str(x), convert_to_tensor=True))\n",
    "    candidates[\"score\"] = candidates[\"desc_emb\"].apply(lambda emb: util.cos_sim(claim_emb, emb).item())\n",
    "\n",
    "    top1 = candidates.sort_values(\"score\", ascending=False).iloc[0][\"maingroup\"]\n",
    "    return normalize_ipc(top1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4vYIWXMsHU4f"
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "results = []\n",
    "\n",
    "for idx, row in tqdm(sample_eval.iterrows(), total=len(sample_eval)):\n",
    "    claim = row[\"claim\"]\n",
    "    label = normalize_ipc(row[\"label\"])\n",
    "\n",
    "    try:\n",
    "        pred = predict_ipc(claim, openai_client, ipc_df, sbert_model, gpt_model_name)\n",
    "        is_correct = (pred == label)\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "    except Exception as e:\n",
    "        pred = f\"error: {e}\"\n",
    "        is_correct = False\n",
    "\n",
    "    results.append({\n",
    "        \"claim\": claim,\n",
    "        \"label\": label,\n",
    "        \"predicted\": pred,\n",
    "        \"correct\": is_correct\n",
    "    })\n",
    "\n",
    "accuracy = correct / len(sample_eval)\n",
    "print(f\"✅ Top-1 정확도: {accuracy:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPnpVaOjfIVQRTsMafP1fE1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
